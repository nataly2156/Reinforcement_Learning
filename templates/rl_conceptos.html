{% extends "base.html" %}

{% block title %}Conceptos B√°sicos - RL{% endblock %}

{% block content %}
<h1 class="text-center mb-5">üìö Conceptos B√°sicos del Aprendizaje por Refuerzo</h1>

<div class="mb-5">
    <h2>1. ¬øQu√© es el Aprendizaje por Refuerzo?</h2>
    <p>El <strong>Aprendizaje por Refuerzo (Reinforcement Learning, RL)</strong> es un paradigma del aprendizaje autom√°tico donde un agente aprende a tomar decisiones mediante la interacci√≥n con un entorno. A diferencia del aprendizaje supervisado, donde se entrena con datos etiquetados, o el aprendizaje no supervisado, que busca patrones en datos sin etiquetar, el RL aprende a trav√©s de un sistema de recompensas y castigos.</p>
    
    <div class="alert alert-info">
        <strong>üí° Diferencias clave:</strong>
        <ul class="mb-0">
            <li><strong>Aprendizaje Supervisado:</strong> Aprende de ejemplos etiquetados (entrada ‚Üí salida correcta)</li>
            <li><strong>Aprendizaje No Supervisado:</strong> Encuentra patrones en datos sin etiquetar</li>
            <li><strong>Aprendizaje por Refuerzo:</strong> Aprende mediante prueba y error, maximizando recompensas acumuladas</li>
        </ul>
    </div>
</div>

<div class="mb-5">
    <h2>2. Componentes del Modelo RL</h2>
    <div class="row">
        <div class="col-md-6 mb-3">
            <div class="card">
                <div class="card-body">
                    <h5 class="card-title">ü§ñ Agente</h5>
                    <p class="card-text">Entidad que toma decisiones y aprende. Es quien ejecuta acciones en el entorno.</p>
                </div>
            </div>
        </div>
        <div class="col-md-6 mb-3">
            <div class="card">
                <div class="card-body">
                    <h5 class="card-title">üåç Entorno</h5>
                    <p class="card-text">El mundo en el que el agente opera. Define las reglas y din√°micas del problema.</p>
                </div>
            </div>
        </div>
        <div class="col-md-6 mb-3">
            <div class="card">
                <div class="card-body">
                    <h5 class="card-title">üìç Estados</h5>
                    <p class="card-text">Representaci√≥n de la situaci√≥n actual del agente en el entorno.</p>
                </div>
            </div>
        </div>
        <div class="col-md-6 mb-3">
            <div class="card">
                <div class="card-body">
                    <h5 class="card-title">‚ö° Acciones</h5>
                    <p class="card-text">Conjunto de movimientos o decisiones que el agente puede ejecutar.</p>
                </div>
            </div>
        </div>
        <div class="col-md-6 mb-3">
            <div class="card">
                <div class="card-body">
                    <h5 class="card-title">üéÅ Recompensas</h5>
                    <p class="card-text">Se√±al num√©rica que indica qu√© tan buena fue una acci√≥n en un estado dado.</p>
                </div>
            </div>
        </div>
        <div class="col-md-6 mb-3">
            <div class="card">
                <div class="card-body">
                    <h5 class="card-title">üéØ Pol√≠tica</h5>
                    <p class="card-text">Estrategia que define qu√© acci√≥n tomar en cada estado.</p>
                </div>
            </div>
        </div>
    </div>
</div>

<div class="mb-5">
    <h2>3. Ciclo de Aprendizaje</h2>
    <p>El proceso de aprendizaje en RL sigue un ciclo continuo:</p>
    <ol>
        <li><strong>Observar</strong> el estado actual del entorno</li>
        <li><strong>Seleccionar</strong> una acci√≥n bas√°ndose en la pol√≠tica actual</li>
        <li><strong>Ejecutar</strong> la acci√≥n en el entorno</li>
        <li><strong>Recibir</strong> una recompensa y observar el nuevo estado</li>
        <li><strong>Actualizar</strong> el conocimiento (Q-table o red neuronal)</li>
    </ol>
    
    <h4 class="mt-4">‚öñÔ∏è Exploraci√≥n vs. Explotaci√≥n</h4>
    <p>Uno de los desaf√≠os centrales en RL es balancear:</p>
    <ul>
        <li><strong>Exploraci√≥n:</strong> Probar nuevas acciones para descubrir mejores estrategias</li>
        <li><strong>Explotaci√≥n:</strong> Usar el conocimiento actual para maximizar recompensas</li>
    </ul>
    <p>La estrategia Œµ-greedy es com√∫n: con probabilidad Œµ se explora (acci√≥n aleatoria), y con probabilidad (1-Œµ) se explota (mejor acci√≥n conocida).</p>
    
    <h4 class="mt-4">üí∞ Retorno Acumulado y Descuento Temporal</h4>
    <p>El objetivo del agente es maximizar el <strong>retorno acumulado</strong>, que es la suma de todas las recompensas futuras, aplicando un <strong>factor de descuento Œ≥</strong> (gamma) que valora m√°s las recompensas inmediatas que las lejanas.</p>
</div>

<div class="mb-5">
    <h2>4. Algoritmos Principales</h2>
    
    <h4>üî∑ Q-Learning</h4>
    <p>Algoritmo off-policy que aprende el valor √≥ptimo de pares estado-acci√≥n. Actualiza una tabla Q usando la ecuaci√≥n de Bellman:</p>
    <div class="alert alert-secondary">
        Q(s,a) ‚Üê Q(s,a) + Œ±[r + Œ≥ max Q(s',a') - Q(s,a)]
    </div>
    <p><strong>Aplicaci√≥n:</strong> Problemas con espacios de estados discretos y peque√±os (navegaci√≥n, juegos simples).</p>
    
    <h4 class="mt-4">üî∂ SARSA</h4>
    <p>Similar a Q-Learning pero on-policy: actualiza bas√°ndose en la acci√≥n realmente tomada, no en la mejor acci√≥n posible.</p>
    <p><strong>Aplicaci√≥n:</strong> Cuando se requiere una pol√≠tica m√°s conservadora y segura.</p>
    
    <h4 class="mt-4">üî∑ Deep Q-Network (DQN)</h4>
    <p>Extensi√≥n de Q-Learning que usa redes neuronales profundas para aproximar la funci√≥n Q, permitiendo trabajar con espacios de estados grandes y continuos.</p>
    <p><strong>Aplicaci√≥n:</strong> Videojuegos complejos (Atari), rob√≥tica, sistemas de control avanzados.</p>
</div>

<div class="mb-5">
    <h2>5. Buenas Pr√°cticas</h2>
    <ul>
        <li><strong>Normalizaci√≥n de recompensas:</strong> Mantener las recompensas en rangos manejables</li>
        <li><strong>Tasa de aprendizaje adaptativa:</strong> Reducir Œ± gradualmente para estabilizar el aprendizaje</li>
        <li><strong>Exploraci√≥n decreciente:</strong> Reducir Œµ con el tiempo (de 1.0 a 0.01)</li>
        <li><strong>Experience Replay:</strong> Almacenar y reutilizar experiencias pasadas (DQN)</li>
        <li><strong>Target Networks:</strong> Usar redes objetivo separadas para estabilizar el aprendizaje</li>
        <li><strong>Convergencia:</strong> Monitorear la recompensa promedio para detectar convergencia</li>
        <li><strong>Generalizaci√≥n:</strong> Probar el agente en escenarios variados para verificar robustez</li>
    </ul>
</div>

<div class="mb-5">
    <h2>6. Referencias Bibliogr√°ficas (APA 7)</h2>
    <div class="alert alert-light">
        <p><strong>Sutton, R. S., & Barto, A. G. (2018).</strong> <em>Reinforcement learning: An introduction</em> (2nd ed.). MIT Press.</p>
        
        <p><strong>Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., Petersen, S., Beattie, C., Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wierstra, D., Legg, S., & Hassabis, D. (2015).</strong> Human-level control through deep reinforcement learning. <em>Nature, 518</em>(7540), 529‚Äì533. https://doi.org/10.1038/nature14236</p>
        
        <p><strong>Watkins, C. J. C. H., & Dayan, P. (1992).</strong> Q-learning. <em>Machine Learning, 8</em>(3), 279‚Äì292. https://doi.org/10.1007/BF00992698</p>
    </div>
</div>

<div class="text-center mt-5">
    <a href="/rl/practico" class="btn btn-primary btn-lg">Ir al Caso Pr√°ctico üéÆ</a>
</div>
{% endblock %}